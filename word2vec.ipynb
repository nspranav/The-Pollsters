{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy import sparse\n",
    "from os.path import expanduser\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [word.strip() for word in open('stop_words.txt').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_tokenizer(str_input):\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
    "    words = [porter_stemmer.stem(word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dems.txt', 'r',encoding=\"utf-8\") as file:\n",
    "    dem_text = [line.strip('\\n') for line in file]\n",
    "with open('gop.txt', 'r',encoding=\"utf-8\") as file:\n",
    "    gop_text = [line.strip('\\n') for line in file]\n",
    "with open('NonPolitical.txt', 'r',encoding=\"utf-8\") as file:\n",
    "    nonp_text = [line.strip('\\n') for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem=np.array(dem_text)\n",
    "gop=np.array(gop_text)\n",
    "nonp=np.array(nonp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_df = pd.DataFrame({'tweet': dem})\n",
    "dem_df['label']=0\n",
    "gop_df = pd.DataFrame({'tweet': gop})\n",
    "gop_df['label']=1\n",
    "nonp_df = pd.DataFrame({'tweet': nonp})\n",
    "nonp_df['label']=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=[dem_df,gop_df,nonp_df]\n",
    "tweets_df=pd.concat(tweets,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc=re.sub(r'-',' ',doc).strip()\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc=re.sub(r'#','',doc).strip() #removing #symbol\n",
    "    doc=re.sub(r'RT[\\s]+','',doc).strip()\n",
    "    doc = re.sub(r'http[a-zA-Z]*', '', doc).strip()\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_dem=normalize_corpus(dem_df['tweet'])\n",
    "norm_gop=normalize_corpus(gop_df['tweet'])\n",
    "norm_nonp=normalize_corpus(nonp_df['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_tweets=np.concatenate((norm_dem, norm_gop,norm_nonp), axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "# tokenize sentences in corpus\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "tokenized_corpus = [wpt.tokenize(document) for document in norm_tweets]\n",
    "\n",
    "# Set values for various parameters\n",
    "feature_size = 100    # Word vector dimensionality  \n",
    "window_context = 30          # Context window size                                                                                    \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "sample = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "w2v_model = word2vec.Word2Vec(tokenized_corpus, size=feature_size, \n",
    "                          window=window_context, min_count=min_word_count,\n",
    "                          sample=sample, iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.623068</td>\n",
       "      <td>0.146551</td>\n",
       "      <td>1.229251</td>\n",
       "      <td>1.251837</td>\n",
       "      <td>0.409443</td>\n",
       "      <td>0.894555</td>\n",
       "      <td>-1.199981</td>\n",
       "      <td>-0.364912</td>\n",
       "      <td>0.151711</td>\n",
       "      <td>0.145984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124447</td>\n",
       "      <td>-0.123198</td>\n",
       "      <td>-1.012831</td>\n",
       "      <td>0.100166</td>\n",
       "      <td>-0.052910</td>\n",
       "      <td>-0.252355</td>\n",
       "      <td>-0.955425</td>\n",
       "      <td>0.119040</td>\n",
       "      <td>-0.453537</td>\n",
       "      <td>-0.515122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.103010</td>\n",
       "      <td>-0.501528</td>\n",
       "      <td>0.322952</td>\n",
       "      <td>0.373686</td>\n",
       "      <td>0.524534</td>\n",
       "      <td>0.221246</td>\n",
       "      <td>-1.325071</td>\n",
       "      <td>-1.565196</td>\n",
       "      <td>-1.164852</td>\n",
       "      <td>0.436400</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.289928</td>\n",
       "      <td>0.037972</td>\n",
       "      <td>-1.103589</td>\n",
       "      <td>-2.356120</td>\n",
       "      <td>1.459980</td>\n",
       "      <td>1.663123</td>\n",
       "      <td>0.033925</td>\n",
       "      <td>-0.056120</td>\n",
       "      <td>0.144457</td>\n",
       "      <td>-2.819482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.907772</td>\n",
       "      <td>0.068927</td>\n",
       "      <td>-0.601583</td>\n",
       "      <td>-0.504977</td>\n",
       "      <td>1.455380</td>\n",
       "      <td>-0.660430</td>\n",
       "      <td>-0.488723</td>\n",
       "      <td>0.062725</td>\n",
       "      <td>0.549753</td>\n",
       "      <td>-0.325872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.724763</td>\n",
       "      <td>0.735749</td>\n",
       "      <td>-0.724880</td>\n",
       "      <td>-0.405693</td>\n",
       "      <td>0.746892</td>\n",
       "      <td>-0.523164</td>\n",
       "      <td>-1.124800</td>\n",
       "      <td>-0.153841</td>\n",
       "      <td>0.086592</td>\n",
       "      <td>-0.543952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.768057</td>\n",
       "      <td>-1.565644</td>\n",
       "      <td>0.586879</td>\n",
       "      <td>1.118854</td>\n",
       "      <td>1.048649</td>\n",
       "      <td>-1.426532</td>\n",
       "      <td>0.848696</td>\n",
       "      <td>-0.248470</td>\n",
       "      <td>-1.019311</td>\n",
       "      <td>0.766040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.988525</td>\n",
       "      <td>-0.076406</td>\n",
       "      <td>0.911665</td>\n",
       "      <td>-1.336391</td>\n",
       "      <td>-0.289586</td>\n",
       "      <td>-0.077546</td>\n",
       "      <td>-0.890714</td>\n",
       "      <td>-1.463279</td>\n",
       "      <td>-1.065610</td>\n",
       "      <td>-1.757756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.063766</td>\n",
       "      <td>-1.428503</td>\n",
       "      <td>0.106107</td>\n",
       "      <td>1.333769</td>\n",
       "      <td>-0.197847</td>\n",
       "      <td>0.250676</td>\n",
       "      <td>-0.422370</td>\n",
       "      <td>-0.868437</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.174691</td>\n",
       "      <td>...</td>\n",
       "      <td>0.615044</td>\n",
       "      <td>-0.407492</td>\n",
       "      <td>-0.706851</td>\n",
       "      <td>-1.284176</td>\n",
       "      <td>1.855251</td>\n",
       "      <td>-0.994677</td>\n",
       "      <td>-0.715710</td>\n",
       "      <td>0.045751</td>\n",
       "      <td>-1.571971</td>\n",
       "      <td>-1.571614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51259</td>\n",
       "      <td>0.364222</td>\n",
       "      <td>-0.544441</td>\n",
       "      <td>-0.597272</td>\n",
       "      <td>-0.031771</td>\n",
       "      <td>0.033734</td>\n",
       "      <td>-0.620527</td>\n",
       "      <td>-0.113756</td>\n",
       "      <td>0.366899</td>\n",
       "      <td>-1.430731</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.464129</td>\n",
       "      <td>-0.418852</td>\n",
       "      <td>-0.097145</td>\n",
       "      <td>1.201959</td>\n",
       "      <td>-0.656091</td>\n",
       "      <td>-0.108371</td>\n",
       "      <td>0.592180</td>\n",
       "      <td>1.446526</td>\n",
       "      <td>1.392030</td>\n",
       "      <td>1.149763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51260</td>\n",
       "      <td>-0.147590</td>\n",
       "      <td>-0.830328</td>\n",
       "      <td>-1.581300</td>\n",
       "      <td>0.064772</td>\n",
       "      <td>-0.442637</td>\n",
       "      <td>0.003115</td>\n",
       "      <td>-0.088177</td>\n",
       "      <td>-0.936537</td>\n",
       "      <td>-0.803143</td>\n",
       "      <td>1.187103</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.070562</td>\n",
       "      <td>-0.300543</td>\n",
       "      <td>-1.019120</td>\n",
       "      <td>-0.142281</td>\n",
       "      <td>0.026141</td>\n",
       "      <td>0.244655</td>\n",
       "      <td>-0.484407</td>\n",
       "      <td>-0.483130</td>\n",
       "      <td>1.408323</td>\n",
       "      <td>-0.542081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51261</td>\n",
       "      <td>1.559632</td>\n",
       "      <td>0.061646</td>\n",
       "      <td>1.816210</td>\n",
       "      <td>0.433380</td>\n",
       "      <td>1.145361</td>\n",
       "      <td>-0.310361</td>\n",
       "      <td>0.206753</td>\n",
       "      <td>-0.628503</td>\n",
       "      <td>-1.634803</td>\n",
       "      <td>0.886144</td>\n",
       "      <td>...</td>\n",
       "      <td>1.483441</td>\n",
       "      <td>1.072332</td>\n",
       "      <td>-0.043770</td>\n",
       "      <td>-0.725508</td>\n",
       "      <td>0.974429</td>\n",
       "      <td>0.802671</td>\n",
       "      <td>-0.367423</td>\n",
       "      <td>0.034357</td>\n",
       "      <td>-0.199419</td>\n",
       "      <td>0.211125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51262</td>\n",
       "      <td>0.415252</td>\n",
       "      <td>-0.550736</td>\n",
       "      <td>-1.139472</td>\n",
       "      <td>0.052513</td>\n",
       "      <td>0.009047</td>\n",
       "      <td>-0.704693</td>\n",
       "      <td>-0.561117</td>\n",
       "      <td>-1.250706</td>\n",
       "      <td>-0.694180</td>\n",
       "      <td>0.245998</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.258692</td>\n",
       "      <td>-0.488286</td>\n",
       "      <td>0.363064</td>\n",
       "      <td>0.077520</td>\n",
       "      <td>0.125586</td>\n",
       "      <td>-0.246778</td>\n",
       "      <td>0.088027</td>\n",
       "      <td>0.411652</td>\n",
       "      <td>1.572580</td>\n",
       "      <td>0.287683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51263</td>\n",
       "      <td>-0.059427</td>\n",
       "      <td>1.669401</td>\n",
       "      <td>-0.034569</td>\n",
       "      <td>-0.877463</td>\n",
       "      <td>-0.818822</td>\n",
       "      <td>0.439086</td>\n",
       "      <td>-1.229530</td>\n",
       "      <td>-0.863485</td>\n",
       "      <td>-0.041405</td>\n",
       "      <td>0.712997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.237576</td>\n",
       "      <td>1.744323</td>\n",
       "      <td>0.327651</td>\n",
       "      <td>-0.781524</td>\n",
       "      <td>0.628325</td>\n",
       "      <td>0.641036</td>\n",
       "      <td>-0.047714</td>\n",
       "      <td>-0.687447</td>\n",
       "      <td>0.828333</td>\n",
       "      <td>-0.340443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51264 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "0      0.623068  0.146551  1.229251  1.251837  0.409443  0.894555 -1.199981   \n",
       "1     -0.103010 -0.501528  0.322952  0.373686  0.524534  0.221246 -1.325071   \n",
       "2     -0.907772  0.068927 -0.601583 -0.504977  1.455380 -0.660430 -0.488723   \n",
       "3     -0.768057 -1.565644  0.586879  1.118854  1.048649 -1.426532  0.848696   \n",
       "4     -0.063766 -1.428503  0.106107  1.333769 -0.197847  0.250676 -0.422370   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "51259  0.364222 -0.544441 -0.597272 -0.031771  0.033734 -0.620527 -0.113756   \n",
       "51260 -0.147590 -0.830328 -1.581300  0.064772 -0.442637  0.003115 -0.088177   \n",
       "51261  1.559632  0.061646  1.816210  0.433380  1.145361 -0.310361  0.206753   \n",
       "51262  0.415252 -0.550736 -1.139472  0.052513  0.009047 -0.704693 -0.561117   \n",
       "51263 -0.059427  1.669401 -0.034569 -0.877463 -0.818822  0.439086 -1.229530   \n",
       "\n",
       "             7         8         9   ...        90        91        92  \\\n",
       "0     -0.364912  0.151711  0.145984  ...  0.124447 -0.123198 -1.012831   \n",
       "1     -1.565196 -1.164852  0.436400  ... -0.289928  0.037972 -1.103589   \n",
       "2      0.062725  0.549753 -0.325872  ...  0.724763  0.735749 -0.724880   \n",
       "3     -0.248470 -1.019311  0.766040  ...  0.988525 -0.076406  0.911665   \n",
       "4     -0.868437  0.181162  0.174691  ...  0.615044 -0.407492 -0.706851   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "51259  0.366899 -1.430731  0.014100  ...  0.464129 -0.418852 -0.097145   \n",
       "51260 -0.936537 -0.803143  1.187103  ... -1.070562 -0.300543 -1.019120   \n",
       "51261 -0.628503 -1.634803  0.886144  ...  1.483441  1.072332 -0.043770   \n",
       "51262 -1.250706 -0.694180  0.245998  ... -0.258692 -0.488286  0.363064   \n",
       "51263 -0.863485 -0.041405  0.712997  ...  0.237576  1.744323  0.327651   \n",
       "\n",
       "             93        94        95        96        97        98        99  \n",
       "0      0.100166 -0.052910 -0.252355 -0.955425  0.119040 -0.453537 -0.515122  \n",
       "1     -2.356120  1.459980  1.663123  0.033925 -0.056120  0.144457 -2.819482  \n",
       "2     -0.405693  0.746892 -0.523164 -1.124800 -0.153841  0.086592 -0.543952  \n",
       "3     -1.336391 -0.289586 -0.077546 -0.890714 -1.463279 -1.065610 -1.757756  \n",
       "4     -1.284176  1.855251 -0.994677 -0.715710  0.045751 -1.571971 -1.571614  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "51259  1.201959 -0.656091 -0.108371  0.592180  1.446526  1.392030  1.149763  \n",
       "51260 -0.142281  0.026141  0.244655 -0.484407 -0.483130  1.408323 -0.542081  \n",
       "51261 -0.725508  0.974429  0.802671 -0.367423  0.034357 -0.199419  0.211125  \n",
       "51262  0.077520  0.125586 -0.246778  0.088027  0.411652  1.572580  0.287683  \n",
       "51263 -0.781524  0.628325  0.641036 -0.047714 -0.687447  0.828333 -0.340443  \n",
       "\n",
       "[51264 rows x 100 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector\n",
    "    \n",
    "   \n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "# get document level embeddings\n",
    "w2v_feature_array = averaged_word_vectorizer(corpus=tokenized_corpus, model=w2v_model,\n",
    "                                             num_features=feature_size)\n",
    "tweet_w2c=pd.DataFrame(w2v_feature_array)\n",
    "tweet_w2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_w2c.to_csv(\"word2vector.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetw2c_df=pd.concat([tweet_w2c,tweets_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.623068</td>\n",
       "      <td>0.146551</td>\n",
       "      <td>1.229251</td>\n",
       "      <td>1.251837</td>\n",
       "      <td>0.409443</td>\n",
       "      <td>0.894555</td>\n",
       "      <td>-1.199981</td>\n",
       "      <td>-0.364912</td>\n",
       "      <td>0.151711</td>\n",
       "      <td>0.145984</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.012831</td>\n",
       "      <td>0.100166</td>\n",
       "      <td>-0.052910</td>\n",
       "      <td>-0.252355</td>\n",
       "      <td>-0.955425</td>\n",
       "      <td>0.119040</td>\n",
       "      <td>-0.453537</td>\n",
       "      <td>-0.515122</td>\n",
       "      <td>This week @senatemajldr said workers don’t nee...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.103010</td>\n",
       "      <td>-0.501528</td>\n",
       "      <td>0.322952</td>\n",
       "      <td>0.373686</td>\n",
       "      <td>0.524534</td>\n",
       "      <td>0.221246</td>\n",
       "      <td>-1.325071</td>\n",
       "      <td>-1.565196</td>\n",
       "      <td>-1.164852</td>\n",
       "      <td>0.436400</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.103589</td>\n",
       "      <td>-2.356120</td>\n",
       "      <td>1.459980</td>\n",
       "      <td>1.663123</td>\n",
       "      <td>0.033925</td>\n",
       "      <td>-0.056120</td>\n",
       "      <td>0.144457</td>\n",
       "      <td>-2.819482</td>\n",
       "      <td>Health care professionals are on the front lin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.907772</td>\n",
       "      <td>0.068927</td>\n",
       "      <td>-0.601583</td>\n",
       "      <td>-0.504977</td>\n",
       "      <td>1.455380</td>\n",
       "      <td>-0.660430</td>\n",
       "      <td>-0.488723</td>\n",
       "      <td>0.062725</td>\n",
       "      <td>0.549753</td>\n",
       "      <td>-0.325872</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.724880</td>\n",
       "      <td>-0.405693</td>\n",
       "      <td>0.746892</td>\n",
       "      <td>-0.523164</td>\n",
       "      <td>-1.124800</td>\n",
       "      <td>-0.153841</td>\n",
       "      <td>0.086592</td>\n",
       "      <td>-0.543952</td>\n",
       "      <td>RT @SeemaNanda: Good to see @Google signal a c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.768057</td>\n",
       "      <td>-1.565644</td>\n",
       "      <td>0.586879</td>\n",
       "      <td>1.118854</td>\n",
       "      <td>1.048649</td>\n",
       "      <td>-1.426532</td>\n",
       "      <td>0.848696</td>\n",
       "      <td>-0.248470</td>\n",
       "      <td>-1.019311</td>\n",
       "      <td>0.766040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.911665</td>\n",
       "      <td>-1.336391</td>\n",
       "      <td>-0.289586</td>\n",
       "      <td>-0.077546</td>\n",
       "      <td>-0.890714</td>\n",
       "      <td>-1.463279</td>\n",
       "      <td>-1.065610</td>\n",
       "      <td>-1.757756</td>\n",
       "      <td>Republicans keep admitting that voter suppress...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.063766</td>\n",
       "      <td>-1.428503</td>\n",
       "      <td>0.106107</td>\n",
       "      <td>1.333769</td>\n",
       "      <td>-0.197847</td>\n",
       "      <td>0.250676</td>\n",
       "      <td>-0.422370</td>\n",
       "      <td>-0.868437</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.174691</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.706851</td>\n",
       "      <td>-1.284176</td>\n",
       "      <td>1.855251</td>\n",
       "      <td>-0.994677</td>\n",
       "      <td>-0.715710</td>\n",
       "      <td>0.045751</td>\n",
       "      <td>-1.571971</td>\n",
       "      <td>-1.571614</td>\n",
       "      <td>RT @SpeakerPelosi: The Congress has so far pas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51259</td>\n",
       "      <td>0.364222</td>\n",
       "      <td>-0.544441</td>\n",
       "      <td>-0.597272</td>\n",
       "      <td>-0.031771</td>\n",
       "      <td>0.033734</td>\n",
       "      <td>-0.620527</td>\n",
       "      <td>-0.113756</td>\n",
       "      <td>0.366899</td>\n",
       "      <td>-1.430731</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.097145</td>\n",
       "      <td>1.201959</td>\n",
       "      <td>-0.656091</td>\n",
       "      <td>-0.108371</td>\n",
       "      <td>0.592180</td>\n",
       "      <td>1.446526</td>\n",
       "      <td>1.392030</td>\n",
       "      <td>1.149763</td>\n",
       "      <td>RT @RecordingAcad: Who is nominated in the Gen...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51260</td>\n",
       "      <td>-0.147590</td>\n",
       "      <td>-0.830328</td>\n",
       "      <td>-1.581300</td>\n",
       "      <td>0.064772</td>\n",
       "      <td>-0.442637</td>\n",
       "      <td>0.003115</td>\n",
       "      <td>-0.088177</td>\n",
       "      <td>-0.936537</td>\n",
       "      <td>-0.803143</td>\n",
       "      <td>1.187103</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.019120</td>\n",
       "      <td>-0.142281</td>\n",
       "      <td>0.026141</td>\n",
       "      <td>0.244655</td>\n",
       "      <td>-0.484407</td>\n",
       "      <td>-0.483130</td>\n",
       "      <td>1.408323</td>\n",
       "      <td>-0.542081</td>\n",
       "      <td>RT @WSJ: Instagram users can now turn off comm...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51261</td>\n",
       "      <td>1.559632</td>\n",
       "      <td>0.061646</td>\n",
       "      <td>1.816210</td>\n",
       "      <td>0.433380</td>\n",
       "      <td>1.145361</td>\n",
       "      <td>-0.310361</td>\n",
       "      <td>0.206753</td>\n",
       "      <td>-0.628503</td>\n",
       "      <td>-1.634803</td>\n",
       "      <td>0.886144</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043770</td>\n",
       "      <td>-0.725508</td>\n",
       "      <td>0.974429</td>\n",
       "      <td>0.802671</td>\n",
       "      <td>-0.367423</td>\n",
       "      <td>0.034357</td>\n",
       "      <td>-0.199419</td>\n",
       "      <td>0.211125</td>\n",
       "      <td>.@valiswiser is on a mission to help people ov...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51262</td>\n",
       "      <td>0.415252</td>\n",
       "      <td>-0.550736</td>\n",
       "      <td>-1.139472</td>\n",
       "      <td>0.052513</td>\n",
       "      <td>0.009047</td>\n",
       "      <td>-0.704693</td>\n",
       "      <td>-0.561117</td>\n",
       "      <td>-1.250706</td>\n",
       "      <td>-0.694180</td>\n",
       "      <td>0.245998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.363064</td>\n",
       "      <td>0.077520</td>\n",
       "      <td>0.125586</td>\n",
       "      <td>-0.246778</td>\n",
       "      <td>0.088027</td>\n",
       "      <td>0.411652</td>\n",
       "      <td>1.572580</td>\n",
       "      <td>0.287683</td>\n",
       "      <td>RT @TechCrunch: Instagram fights abuse with co...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51263</td>\n",
       "      <td>-0.059427</td>\n",
       "      <td>1.669401</td>\n",
       "      <td>-0.034569</td>\n",
       "      <td>-0.877463</td>\n",
       "      <td>-0.818822</td>\n",
       "      <td>0.439086</td>\n",
       "      <td>-1.229530</td>\n",
       "      <td>-0.863485</td>\n",
       "      <td>-0.041405</td>\n",
       "      <td>0.712997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.327651</td>\n",
       "      <td>-0.781524</td>\n",
       "      <td>0.628325</td>\n",
       "      <td>0.641036</td>\n",
       "      <td>-0.047714</td>\n",
       "      <td>-0.687447</td>\n",
       "      <td>0.828333</td>\n",
       "      <td>-0.340443</td>\n",
       "      <td>“The most important thing is to always be your...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51264 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0      0.623068  0.146551  1.229251  1.251837  0.409443  0.894555 -1.199981   \n",
       "1     -0.103010 -0.501528  0.322952  0.373686  0.524534  0.221246 -1.325071   \n",
       "2     -0.907772  0.068927 -0.601583 -0.504977  1.455380 -0.660430 -0.488723   \n",
       "3     -0.768057 -1.565644  0.586879  1.118854  1.048649 -1.426532  0.848696   \n",
       "4     -0.063766 -1.428503  0.106107  1.333769 -0.197847  0.250676 -0.422370   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "51259  0.364222 -0.544441 -0.597272 -0.031771  0.033734 -0.620527 -0.113756   \n",
       "51260 -0.147590 -0.830328 -1.581300  0.064772 -0.442637  0.003115 -0.088177   \n",
       "51261  1.559632  0.061646  1.816210  0.433380  1.145361 -0.310361  0.206753   \n",
       "51262  0.415252 -0.550736 -1.139472  0.052513  0.009047 -0.704693 -0.561117   \n",
       "51263 -0.059427  1.669401 -0.034569 -0.877463 -0.818822  0.439086 -1.229530   \n",
       "\n",
       "              7         8         9  ...        92        93        94  \\\n",
       "0     -0.364912  0.151711  0.145984  ... -1.012831  0.100166 -0.052910   \n",
       "1     -1.565196 -1.164852  0.436400  ... -1.103589 -2.356120  1.459980   \n",
       "2      0.062725  0.549753 -0.325872  ... -0.724880 -0.405693  0.746892   \n",
       "3     -0.248470 -1.019311  0.766040  ...  0.911665 -1.336391 -0.289586   \n",
       "4     -0.868437  0.181162  0.174691  ... -0.706851 -1.284176  1.855251   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "51259  0.366899 -1.430731  0.014100  ... -0.097145  1.201959 -0.656091   \n",
       "51260 -0.936537 -0.803143  1.187103  ... -1.019120 -0.142281  0.026141   \n",
       "51261 -0.628503 -1.634803  0.886144  ... -0.043770 -0.725508  0.974429   \n",
       "51262 -1.250706 -0.694180  0.245998  ...  0.363064  0.077520  0.125586   \n",
       "51263 -0.863485 -0.041405  0.712997  ...  0.327651 -0.781524  0.628325   \n",
       "\n",
       "             95        96        97        98        99  \\\n",
       "0     -0.252355 -0.955425  0.119040 -0.453537 -0.515122   \n",
       "1      1.663123  0.033925 -0.056120  0.144457 -2.819482   \n",
       "2     -0.523164 -1.124800 -0.153841  0.086592 -0.543952   \n",
       "3     -0.077546 -0.890714 -1.463279 -1.065610 -1.757756   \n",
       "4     -0.994677 -0.715710  0.045751 -1.571971 -1.571614   \n",
       "...         ...       ...       ...       ...       ...   \n",
       "51259 -0.108371  0.592180  1.446526  1.392030  1.149763   \n",
       "51260  0.244655 -0.484407 -0.483130  1.408323 -0.542081   \n",
       "51261  0.802671 -0.367423  0.034357 -0.199419  0.211125   \n",
       "51262 -0.246778  0.088027  0.411652  1.572580  0.287683   \n",
       "51263  0.641036 -0.047714 -0.687447  0.828333 -0.340443   \n",
       "\n",
       "                                                   tweet  label  \n",
       "0      This week @senatemajldr said workers don’t nee...      0  \n",
       "1      Health care professionals are on the front lin...      0  \n",
       "2      RT @SeemaNanda: Good to see @Google signal a c...      0  \n",
       "3      Republicans keep admitting that voter suppress...      0  \n",
       "4      RT @SpeakerPelosi: The Congress has so far pas...      0  \n",
       "...                                                  ...    ...  \n",
       "51259  RT @RecordingAcad: Who is nominated in the Gen...      2  \n",
       "51260  RT @WSJ: Instagram users can now turn off comm...      2  \n",
       "51261  .@valiswiser is on a mission to help people ov...      2  \n",
       "51262  RT @TechCrunch: Instagram fights abuse with co...      2  \n",
       "51263  “The most important thing is to always be your...      2  \n",
       "\n",
       "[51264 rows x 102 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetw2c_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tweetw2c_df.drop(['tweet','label'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=tweetw2c_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = BernoulliNB()\n",
    "model = naive_bayes.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.732521847690387"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='newton-cg', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25,random_state = 42) \n",
    "log_classifier = LogisticRegression(multi_class='multinomial',solver ='newton-cg')\n",
    "log_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8238920099875156"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_classifier.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3901,  750,  188],\n",
       "       [ 771, 3696,  233],\n",
       "       [ 132,  183, 2962]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3898,  754,  187],\n",
       "       [ 771, 3679,  250],\n",
       "       [ 144,  171, 2962]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "model = LinearSVC()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8223314606741573"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
