{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import  SparkContext\n",
    "from pyspark.sql.functions import col, lower\n",
    "from pyspark.sql import SQLContext\n",
    "import re\n",
    "sc = SparkContext('local','test1')\n",
    "sql = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "dems_df =  sql.read.text(\"dems.txt\")\n",
    "gop_df = sql.read.text(\"gop.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df = dems_df.select(\"value\", lit(1).alias(\"label\")).union(gop_df.select(\"value\", lit(0).alias(\"label\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               value|label|\n",
      "+--------------------+-----+\n",
      "|This week @senate...|    1|\n",
      "|Health care profe...|    1|\n",
      "|RT @SeemaNanda: G...|    1|\n",
      "|Republicans keep ...|    1|\n",
      "|RT @SpeakerPelosi...|    1|\n",
      "|While the preside...|    1|\n",
      "|You are not alone...|    1|\n",
      "|RT @DNCWarRoom: W...|    1|\n",
      "|RT @DNCWarRoom: T...|    1|\n",
      "|RT @DNCWarRoom: T...|    1|\n",
      "|LISTEN. TO. HEALT...|    1|\n",
      "|RT @SeemaNanda: B...|    1|\n",
      "|This is a HUGE wi...|    1|\n",
      "|RT @SenSherrodBro...|    1|\n",
      "|RT @WisDems: Make...|    1|\n",
      "|Trump had warning...|    1|\n",
      "|RT @DemConvention...|    1|\n",
      "|Abortion is healt...|    1|\n",
      "|RT @RepLucyMcBath...|    1|\n",
      "|Get counted. Get ...|    1|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_df.select(\"*\").limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf,lower,col,trim\n",
    "from pyspark.sql.types import FloatType,StringType,IntegerType\n",
    "def clean_text(text):\n",
    "    text=re.sub(r'@[A-Za-z0-9]+','',text).strip() #remove mentions\n",
    "    text=re.sub(r'#','',text).strip() #removing #symbol\n",
    "    text=re.sub(r'RT[\\s]+','',text).strip()\n",
    "    #text=re.sub(r'https?:\\/\\/\\S+','',text)\n",
    "    #text=re.sub(r'\\b(?:(?:https?|ftp)://)?\\w[\\w-]*(?:\\.[\\w-]+)+\\S*', '', text)\n",
    "    text=re.sub(r'[?|$|.|!|;|:|&|\"|,|\"\"*-]','',text).strip()\n",
    "    text=re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)+',\"\",text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_alphnum(text):\n",
    "    t=\"\"\n",
    "    for word in text.split(\" \"):\n",
    "        if (word.isalpha() and len(word)>3):\n",
    "            t=t+\" \"+word\n",
    "    return str(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_len(text):\n",
    "#     t=\" \"\n",
    "#     for word in text.split(\" \"):\n",
    "#         if(len(word)>3):\n",
    "#             t=t+\" \"+word\n",
    "#     return str(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#value\n",
    "clean_udf_str=udf(lambda z: clean_text(z), StringType())\n",
    "corpus_df=corpus_df.select(\"label\",clean_udf_str(\"value\").alias(\"value\"))\n",
    "emoji_udf_str=udf(lambda z: remove_emoji(z), StringType())\n",
    "corpus_df=corpus_df.select(\"label\",emoji_udf_str('value').alias('value'))\n",
    "corpus_df=corpus_df.select(trim(lower(col('value'))).alias(\"value\"),\"label\")\n",
    "\n",
    "#e_udf_str=udf(lambda z: check_alph(z), StringType())\n",
    "#corpus_df=corpus_df.select(\"label\",e_udf_str('value').alias('value'))\n",
    "#f_udf_str=udf(lambda z: check_alphnum(z), StringType())\n",
    "#corpus_df=corpus_df.select(\"label\",f_udf_str('value').alias('value'))\n",
    "#corpus_df=corpus_df.select(trim(col('value')).alias(\"value\"),\"label\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               value|label|\n",
      "+--------------------+-----+\n",
      "|this week  said w...|    1|\n",
      "|health care profe...|    1|\n",
      "|good to see  sign...|    1|\n",
      "|republicans keep ...|    1|\n",
      "|the congress has ...|    1|\n",
      "|while the preside...|    1|\n",
      "|you are not alone...|    1|\n",
      "|well this is conc...|    1|\n",
      "|trump “in the end...|    1|\n",
      "|trump proposed hu...|    1|\n",
      "|listen to health ...|    1|\n",
      "|breaking we  alon...|    1|\n",
      "|this is a huge wi...|    1|\n",
      "|update this is th...|    1|\n",
      "|make sure your vo...|    1|\n",
      "|trump had warning...|    1|\n",
      "|in light of the u...|    1|\n",
      "|abortion is healt...|    1|\n",
      "|why does completi...|    1|\n",
      "|get counted get c...|    1|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_df.select(\"*\").limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = corpus_df.randomSplit([0.75, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|'s actions on dac...|\n",
      "|'s campaign says ...|\n",
      "|(1/3) yesterday i...|\n",
      "|(ca14) hosted “a ...|\n",
      "|(co07) held a gov...|\n",
      "|(fl07) toured ’s ...|\n",
      "|(fl13) met with p...|\n",
      "|(fl14) discussed ...|\n",
      "|(il17) visited go...|\n",
      "|(in07) greeted ov...|\n",
      "|(mi12) attended t...|\n",
      "|(ny03) and 99 ½ y...|\n",
      "|(ny03) watched ma...|\n",
      "|(or01) worked wit...|\n",
      "|(tx18) stood with...|\n",
      "|(tx33) spoke at h...|\n",
      "|(tx33) stopped by...|\n",
      "|(wa10) shadowed t...|\n",
      "|(•_•) lt)   )the ...|\n",
      "|1 day of trump he...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train_df.select(lower(col('value'))).show()\n",
    "test_df.select(\"value\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer, Tokenizer, StopWordsRemover\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"value\", outputCol=\"words\")\n",
    "stop_words_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"words_cleaned\")\n",
    "vectorizer = CountVectorizer(inputCol=\"words_cleaned\", outputCol=\"features\")\n",
    "cleaning_pipeline = Pipeline(stages = [tokenizer,stop_words_remover,vectorizer])\n",
    "#cleaning_pipeline = Pipeline(stages = [stop_words_remover,vectorizer])\n",
    "cleaning_pipeline_model = cleaning_pipeline.fit(corpus_df)\n",
    "cleaned_training_df = cleaning_pipeline_model.transform(train_df)\n",
    "cleaned_testing_df = cleaning_pipeline_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|               value|label|               words|       words_cleaned|            features|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|                    |    1|                  []|                  []|(55380,[3944],[1.0])|\n",
      "|                    |    1|                  []|                  []|(55380,[3944],[1.0])|\n",
      "|                    |    1|                  []|                  []|(55380,[3944],[1.0])|\n",
      "|                    |    1|                  []|                  []|(55380,[3944],[1.0])|\n",
      "|                    |    1|                  []|                  []|(55380,[3944],[1.0])|\n",
      "|                    |    1|                  []|                  []|(55380,[3944],[1.0])|\n",
      "|                    |    1|                  []|                  []|(55380,[3944],[1.0])|\n",
      "|                    |    1|                  []|                  []|(55380,[3944],[1.0])|\n",
      "|                    |    1|                  []|                  []|(55380,[3944],[1.0])|\n",
      "|'read transcript'...|    1|['read, transcrip...|['read, transcrip...|(55380,[0,1,154,3...|\n",
      "|(1/3) yesterday c...|    1|[(1/3), yesterday...|[(1/3), yesterday...|(55380,[204,1077,...|\n",
      "|(ca06) participat...|    1|[(ca06), particip...|[(ca06), particip...|(55380,[957,977,1...|\n",
      "|(ca14) hosted con...|    1|[(ca14), hosted, ...|[(ca14), hosted, ...|(55380,[32,42,343...|\n",
      "|(ca14) hosted con...|    1|[(ca14), hosted, ...|[(ca14), hosted, ...|(55380,[32,42,343...|\n",
      "|(ca14) with lucy ...|    1|[(ca14), with, lu...|[(ca14), lucy, mi...|(55380,[547,977,1...|\n",
      "|(ca15) worked shi...|    1|[(ca15), worked, ...|[(ca15), worked, ...|(55380,[208,275,2...|\n",
      "|(ca24) joined lui...|    1|[(ca24), joined, ...|[(ca24), joined, ...|(55380,[139,260,3...|\n",
      "|(ca26) with veter...|    1|[(ca26), with, ve...|[(ca26), veteran,...|(55380,[30,152,20...|\n",
      "|(ca32) participat...|    1|[(ca32), particip...|[(ca32), particip...|(55380,[977,1014,...|\n",
      "|(ca33) visited sa...|    1|[(ca33), visited,...|[(ca33), visited,...|(55380,[5,76,81,2...|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_training_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_training_df.toPandas().to_csv(\"cleanedtraining.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "naive_bayes = NaiveBayes(featuresCol=\"features\", labelCol = \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes_model = naive_bayes.fit(cleaned_training_df)\n",
    "predictions_df = naive_bayes_model.transform(cleaned_testing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|            features|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|   (45487,[0],[1.0])|    1|       0.0|\n",
      "|   (45487,[0],[1.0])|    1|       0.0|\n",
      "|(45487,[0,2,3,323...|    1|       0.0|\n",
      "|(45487,[0,76,80,5...|    1|       0.0|\n",
      "|(45487,[0,41,122,...|    1|       1.0|\n",
      "|(45487,[0,29,107,...|    1|       1.0|\n",
      "|(45487,[0,13,14,3...|    1|       1.0|\n",
      "|(45487,[1,4,24,68...|    1|       1.0|\n",
      "|(45487,[7,10,12,3...|    1|       1.0|\n",
      "|(45487,[7,10,12,3...|    1|       1.0|\n",
      "|(45487,[12,18,24,...|    1|       1.0|\n",
      "|(45487,[0,9,52,10...|    1|       1.0|\n",
      "|(45487,[0,24,54,8...|    1|       1.0|\n",
      "|(45487,[0,29,283,...|    1|       1.0|\n",
      "|(45487,[0,29,283,...|    1|       1.0|\n",
      "|(45487,[5,18,31,6...|    1|       1.0|\n",
      "|(45487,[3,12,46,6...|    1|       1.0|\n",
      "|(45487,[5,101,141...|    1|       1.0|\n",
      "|(45487,[0,2,13,15...|    1|       1.0|\n",
      "|(45487,[4,24,68,9...|    1|       1.0|\n",
      "+--------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_df.select(\"features\",\"label\",\"prediction\").limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6744747899159664"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "eval = MulticlassClassificationEvaluator(labelCol='label',predictionCol='prediction', metricName = 'accuracy')\n",
    "eval.evaluate(predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(45487,[23,28,43,...|\n",
      "|(45487,[0,12,21,1...|\n",
      "|(45487,[0,4,52,21...|\n",
      "|(45487,[0,55,143,...|\n",
      "+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_training_df.select(\"features\").show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
