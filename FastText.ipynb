{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy import sparse\n",
    "from os.path import expanduser\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [word.strip() for word in open('stop_words.txt').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_tokenizer(str_input):\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
    "    words = [porter_stemmer.stem(word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dems.txt', 'r',encoding=\"utf-8\") as file:\n",
    "    dem_text = [line.strip('\\n') for line in file]\n",
    "with open('gop.txt', 'r',encoding=\"utf-8\") as file:\n",
    "    gop_text = [line.strip('\\n') for line in file]\n",
    "with open('NonPolitical.txt', 'r',encoding=\"utf-8\") as file:\n",
    "    nonp_text = [line.strip('\\n') for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem=np.array(dem_text)\n",
    "gop=np.array(gop_text)\n",
    "nonp=np.array(nonp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_df = pd.DataFrame({'tweet': dem})\n",
    "dem_df['label']=0\n",
    "gop_df = pd.DataFrame({'tweet': gop})\n",
    "gop_df['label']=1\n",
    "nonp_df = pd.DataFrame({'tweet': nonp})\n",
    "nonp_df['label']=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=[dem_df,gop_df,nonp_df]\n",
    "tweets_df=pd.concat(tweets,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc=re.sub(r'-',' ',doc).strip()\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc=re.sub(r'#','',doc).strip() #removing #symbol\n",
    "    doc=re.sub(r'RT[\\s]+','',doc).strip()\n",
    "    doc = re.sub(r'http[a-zA-Z]*', '', doc).strip()\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_dem=normalize_corpus(dem_df['tweet'])\n",
    "norm_gop=normalize_corpus(gop_df['tweet'])\n",
    "norm_nonp=normalize_corpus(nonp_df['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_tweets=np.concatenate((norm_dem, norm_gop,norm_nonp), axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "tokenized_corpus = [wpt.tokenize(document) for document in norm_tweets]\n",
    "\n",
    "# Set values for various parameters\n",
    "feature_size = 100    # Word vector dimensionality  \n",
    "window_context = 50          # Context window size                                                                                    \n",
    "min_word_count = 5   # Minimum word count                        \n",
    "sample = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "\n",
    "ft_model = FastText(tokenized_corpus, size=feature_size, window=window_context, \n",
    "                    min_count=min_word_count,sample=sample, sg=1, iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.131927</td>\n",
       "      <td>0.072587</td>\n",
       "      <td>0.148973</td>\n",
       "      <td>0.179304</td>\n",
       "      <td>-0.034604</td>\n",
       "      <td>0.055214</td>\n",
       "      <td>-0.215612</td>\n",
       "      <td>0.134485</td>\n",
       "      <td>-0.046035</td>\n",
       "      <td>0.016965</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156378</td>\n",
       "      <td>-0.175986</td>\n",
       "      <td>-0.124601</td>\n",
       "      <td>0.011181</td>\n",
       "      <td>0.096156</td>\n",
       "      <td>0.004070</td>\n",
       "      <td>-0.161479</td>\n",
       "      <td>-0.011840</td>\n",
       "      <td>-0.032572</td>\n",
       "      <td>0.280729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.099689</td>\n",
       "      <td>-0.038240</td>\n",
       "      <td>0.264251</td>\n",
       "      <td>0.122051</td>\n",
       "      <td>-0.050683</td>\n",
       "      <td>-0.116872</td>\n",
       "      <td>-0.250062</td>\n",
       "      <td>-0.112182</td>\n",
       "      <td>-0.007713</td>\n",
       "      <td>0.017609</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060585</td>\n",
       "      <td>0.126893</td>\n",
       "      <td>0.101561</td>\n",
       "      <td>0.145015</td>\n",
       "      <td>0.332763</td>\n",
       "      <td>0.162034</td>\n",
       "      <td>-0.099668</td>\n",
       "      <td>0.137067</td>\n",
       "      <td>0.040090</td>\n",
       "      <td>0.155525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.206245</td>\n",
       "      <td>-0.008839</td>\n",
       "      <td>0.147725</td>\n",
       "      <td>0.090257</td>\n",
       "      <td>0.105787</td>\n",
       "      <td>0.209830</td>\n",
       "      <td>-0.130037</td>\n",
       "      <td>0.031220</td>\n",
       "      <td>-0.268257</td>\n",
       "      <td>0.124062</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078372</td>\n",
       "      <td>-0.027156</td>\n",
       "      <td>-0.065762</td>\n",
       "      <td>0.068860</td>\n",
       "      <td>0.110813</td>\n",
       "      <td>0.082470</td>\n",
       "      <td>-0.117253</td>\n",
       "      <td>0.063413</td>\n",
       "      <td>-0.026084</td>\n",
       "      <td>-0.093323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.014056</td>\n",
       "      <td>-0.129698</td>\n",
       "      <td>0.064914</td>\n",
       "      <td>0.201380</td>\n",
       "      <td>0.015062</td>\n",
       "      <td>0.274785</td>\n",
       "      <td>-0.277500</td>\n",
       "      <td>-0.069255</td>\n",
       "      <td>-0.048947</td>\n",
       "      <td>-0.017775</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.266125</td>\n",
       "      <td>-0.216330</td>\n",
       "      <td>-0.198291</td>\n",
       "      <td>0.158487</td>\n",
       "      <td>0.282376</td>\n",
       "      <td>-0.120763</td>\n",
       "      <td>0.097646</td>\n",
       "      <td>0.085710</td>\n",
       "      <td>0.125607</td>\n",
       "      <td>0.136412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.075753</td>\n",
       "      <td>0.345024</td>\n",
       "      <td>0.068511</td>\n",
       "      <td>0.397493</td>\n",
       "      <td>-0.078619</td>\n",
       "      <td>0.259986</td>\n",
       "      <td>-0.085486</td>\n",
       "      <td>0.007347</td>\n",
       "      <td>-0.084314</td>\n",
       "      <td>0.069333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108897</td>\n",
       "      <td>-0.221188</td>\n",
       "      <td>-0.127309</td>\n",
       "      <td>0.008877</td>\n",
       "      <td>0.190784</td>\n",
       "      <td>0.130552</td>\n",
       "      <td>-0.085027</td>\n",
       "      <td>0.038165</td>\n",
       "      <td>0.173829</td>\n",
       "      <td>0.252243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51259</td>\n",
       "      <td>-0.037639</td>\n",
       "      <td>-0.261345</td>\n",
       "      <td>0.010619</td>\n",
       "      <td>0.086037</td>\n",
       "      <td>0.124329</td>\n",
       "      <td>0.186299</td>\n",
       "      <td>0.104132</td>\n",
       "      <td>0.218581</td>\n",
       "      <td>0.099928</td>\n",
       "      <td>0.060806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115560</td>\n",
       "      <td>0.040928</td>\n",
       "      <td>-0.168453</td>\n",
       "      <td>-0.158409</td>\n",
       "      <td>-0.192831</td>\n",
       "      <td>0.194649</td>\n",
       "      <td>-0.205201</td>\n",
       "      <td>-0.135646</td>\n",
       "      <td>-0.120890</td>\n",
       "      <td>-0.098033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51260</td>\n",
       "      <td>-0.280390</td>\n",
       "      <td>-0.037974</td>\n",
       "      <td>-0.016714</td>\n",
       "      <td>0.058434</td>\n",
       "      <td>0.267959</td>\n",
       "      <td>0.157820</td>\n",
       "      <td>-0.156949</td>\n",
       "      <td>-0.007220</td>\n",
       "      <td>-0.128691</td>\n",
       "      <td>-0.135120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056162</td>\n",
       "      <td>-0.128397</td>\n",
       "      <td>-0.120808</td>\n",
       "      <td>0.104969</td>\n",
       "      <td>-0.281571</td>\n",
       "      <td>0.325738</td>\n",
       "      <td>0.090204</td>\n",
       "      <td>-0.356875</td>\n",
       "      <td>-0.151682</td>\n",
       "      <td>0.059654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51261</td>\n",
       "      <td>-0.085223</td>\n",
       "      <td>-0.027967</td>\n",
       "      <td>0.329221</td>\n",
       "      <td>0.177641</td>\n",
       "      <td>0.340441</td>\n",
       "      <td>0.147450</td>\n",
       "      <td>-0.174177</td>\n",
       "      <td>-0.022423</td>\n",
       "      <td>-0.208540</td>\n",
       "      <td>0.067974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061531</td>\n",
       "      <td>-0.068941</td>\n",
       "      <td>-0.094095</td>\n",
       "      <td>0.293323</td>\n",
       "      <td>0.352616</td>\n",
       "      <td>0.102702</td>\n",
       "      <td>-0.199347</td>\n",
       "      <td>0.018171</td>\n",
       "      <td>0.030810</td>\n",
       "      <td>-0.225234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51262</td>\n",
       "      <td>-0.303152</td>\n",
       "      <td>-0.042140</td>\n",
       "      <td>0.152902</td>\n",
       "      <td>-0.005012</td>\n",
       "      <td>0.231356</td>\n",
       "      <td>-0.016039</td>\n",
       "      <td>0.051919</td>\n",
       "      <td>0.029954</td>\n",
       "      <td>-0.015057</td>\n",
       "      <td>0.041116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131339</td>\n",
       "      <td>0.110444</td>\n",
       "      <td>0.028329</td>\n",
       "      <td>0.071822</td>\n",
       "      <td>-0.187839</td>\n",
       "      <td>-0.027097</td>\n",
       "      <td>-0.036833</td>\n",
       "      <td>-0.097223</td>\n",
       "      <td>-0.192827</td>\n",
       "      <td>0.071055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51263</td>\n",
       "      <td>-0.147999</td>\n",
       "      <td>0.205148</td>\n",
       "      <td>0.130590</td>\n",
       "      <td>0.047697</td>\n",
       "      <td>0.119999</td>\n",
       "      <td>0.159039</td>\n",
       "      <td>-0.185391</td>\n",
       "      <td>0.008372</td>\n",
       "      <td>-0.210826</td>\n",
       "      <td>0.022481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118041</td>\n",
       "      <td>-0.064173</td>\n",
       "      <td>0.052515</td>\n",
       "      <td>0.193036</td>\n",
       "      <td>0.142446</td>\n",
       "      <td>-0.136927</td>\n",
       "      <td>-0.176434</td>\n",
       "      <td>0.012488</td>\n",
       "      <td>-0.139425</td>\n",
       "      <td>0.000449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51264 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "0     -0.131927  0.072587  0.148973  0.179304 -0.034604  0.055214 -0.215612   \n",
       "1     -0.099689 -0.038240  0.264251  0.122051 -0.050683 -0.116872 -0.250062   \n",
       "2     -0.206245 -0.008839  0.147725  0.090257  0.105787  0.209830 -0.130037   \n",
       "3     -0.014056 -0.129698  0.064914  0.201380  0.015062  0.274785 -0.277500   \n",
       "4     -0.075753  0.345024  0.068511  0.397493 -0.078619  0.259986 -0.085486   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "51259 -0.037639 -0.261345  0.010619  0.086037  0.124329  0.186299  0.104132   \n",
       "51260 -0.280390 -0.037974 -0.016714  0.058434  0.267959  0.157820 -0.156949   \n",
       "51261 -0.085223 -0.027967  0.329221  0.177641  0.340441  0.147450 -0.174177   \n",
       "51262 -0.303152 -0.042140  0.152902 -0.005012  0.231356 -0.016039  0.051919   \n",
       "51263 -0.147999  0.205148  0.130590  0.047697  0.119999  0.159039 -0.185391   \n",
       "\n",
       "             7         8         9   ...        90        91        92  \\\n",
       "0      0.134485 -0.046035  0.016965  ...  0.156378 -0.175986 -0.124601   \n",
       "1     -0.112182 -0.007713  0.017609  ...  0.060585  0.126893  0.101561   \n",
       "2      0.031220 -0.268257  0.124062  ... -0.078372 -0.027156 -0.065762   \n",
       "3     -0.069255 -0.048947 -0.017775  ... -0.266125 -0.216330 -0.198291   \n",
       "4      0.007347 -0.084314  0.069333  ...  0.108897 -0.221188 -0.127309   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "51259  0.218581  0.099928  0.060806  ...  0.115560  0.040928 -0.168453   \n",
       "51260 -0.007220 -0.128691 -0.135120  ...  0.056162 -0.128397 -0.120808   \n",
       "51261 -0.022423 -0.208540  0.067974  ...  0.061531 -0.068941 -0.094095   \n",
       "51262  0.029954 -0.015057  0.041116  ...  0.131339  0.110444  0.028329   \n",
       "51263  0.008372 -0.210826  0.022481  ...  0.118041 -0.064173  0.052515   \n",
       "\n",
       "             93        94        95        96        97        98        99  \n",
       "0      0.011181  0.096156  0.004070 -0.161479 -0.011840 -0.032572  0.280729  \n",
       "1      0.145015  0.332763  0.162034 -0.099668  0.137067  0.040090  0.155525  \n",
       "2      0.068860  0.110813  0.082470 -0.117253  0.063413 -0.026084 -0.093323  \n",
       "3      0.158487  0.282376 -0.120763  0.097646  0.085710  0.125607  0.136412  \n",
       "4      0.008877  0.190784  0.130552 -0.085027  0.038165  0.173829  0.252243  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "51259 -0.158409 -0.192831  0.194649 -0.205201 -0.135646 -0.120890 -0.098033  \n",
       "51260  0.104969 -0.281571  0.325738  0.090204 -0.356875 -0.151682  0.059654  \n",
       "51261  0.293323  0.352616  0.102702 -0.199347  0.018171  0.030810 -0.225234  \n",
       "51262  0.071822 -0.187839 -0.027097 -0.036833 -0.097223 -0.192827  0.071055  \n",
       "51263  0.193036  0.142446 -0.136927 -0.176434  0.012488 -0.139425  0.000449  \n",
       "\n",
       "[51264 rows x 100 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector\n",
    "    \n",
    "   \n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "# get document level embeddings\n",
    "ft_feature_array = averaged_word_vectorizer(corpus=tokenized_corpus, model=ft_model,\n",
    "                                             num_features=feature_size)\n",
    "tweet_ft=pd.DataFrame(ft_feature_array)\n",
    "tweet_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_ft.to_csv(\"FastText.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetft_df=pd.concat([tweet_ft,tweets_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x=tweetft_df.drop(['tweet','label'],axis=1)\n",
    "y=tweetft_df['label']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7056803995006242"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "naive_bayes = BernoulliNB()\n",
    "model = naive_bayes.fit(x_train, y_train)\n",
    "y_predictions = model.predict(x_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='newton-cg', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25,random_state = 42) \n",
    "log_classifier = LogisticRegression(multi_class='multinomial',solver ='newton-cg')\n",
    "log_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8205368289637952"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_classifier.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3920,  806,  113],\n",
       "       [ 816, 3693,  191],\n",
       "       [ 186,  188, 2903]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3931,  783,  125],\n",
       "       [ 825, 3669,  206],\n",
       "       [ 183,  184, 2910]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "model = LinearSVC()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8200686641697877"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
