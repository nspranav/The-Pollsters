{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy import sparse\n",
    "from os.path import expanduser\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [word.strip() for word in open('stop_words.txt').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_tokenizer(str_input):\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
    "    words = [porter_stemmer.stem(word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dems.txt', 'r',encoding=\"utf-8\") as file:\n",
    "    dem_text = [line.strip('\\n') for line in file]\n",
    "with open('gop.txt', 'r',encoding=\"utf-8\") as file:\n",
    "    gop_text = [line.strip('\\n') for line in file]\n",
    "with open('NonPolitical.txt', 'r',encoding=\"utf-8\") as file:\n",
    "    nonp_text = [line.strip('\\n') for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem=np.array(dem_text)\n",
    "gop=np.array(gop_text)\n",
    "nonp=np.array(nonp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_df = pd.DataFrame({'tweet': dem})\n",
    "dem_df['label']=0\n",
    "gop_df = pd.DataFrame({'tweet': gop})\n",
    "gop_df['label']=1\n",
    "nonp_df = pd.DataFrame({'tweet': nonp})\n",
    "nonp_df['label']=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=[dem_df,gop_df,nonp_df]\n",
    "tweets_df=pd.concat(tweets,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc=re.sub(r'-',' ',doc).strip()\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc=re.sub(r'#','',doc).strip() #removing #symbol\n",
    "    doc=re.sub(r'RT[\\s]+','',doc).strip()\n",
    "    doc = re.sub(r'http[a-zA-Z]*', '', doc).strip()\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_dem=normalize_corpus(dem_df['tweet'])\n",
    "norm_gop=normalize_corpus(gop_df['tweet'])\n",
    "norm_nonp=normalize_corpus(nonp_df['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_tweets=np.concatenate((norm_dem, norm_gop,norm_nonp), axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-76f3e64309f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m ft_model = FastText(tokenized_corpus, size=feature_size, window=window_context, \n\u001b[1;32m---> 14\u001b[1;33m                     min_count=min_word_count,sample=sample, sg=1, iter=50)\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\fasttext.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, sg, hs, size, alpha, window, min_count, max_vocab_size, word_ngrams, sample, seed, workers, min_alpha, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, min_n, max_n, sorted_vocab, bucket, trim_rule, batch_words, callbacks, compatible_hash)\u001b[0m\n\u001b[0;32m    599\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m             seed=seed, hs=hs, negative=negative, cbow_mean=cbow_mean, min_alpha=min_alpha, fast_version=FAST_VERSION)\n\u001b[0m\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[0;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 763\u001b[1;33m                 end_alpha=self.min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[0;32m    764\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\fasttext.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    920\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    921\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 922\u001b[1;33m             queue_factor=queue_factor, report_delay=report_delay, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    923\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjust_vectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    924\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1081\u001b[1;33m             **kwargs)\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    551\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[0;32m    552\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[0;32m    554\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[1;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[0;32m    487\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[0;32m    488\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             report_delay=report_delay, is_corpus_file_mode=False)\n\u001b[0m\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[1;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m             \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# blocks if workers too slow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# a thread reporting that it finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\queue.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "tokenized_corpus = [wpt.tokenize(document) for document in norm_tweets]\n",
    "\n",
    "# Set values for various parameters\n",
    "feature_size = 100    # Word vector dimensionality  \n",
    "window_context = 30          # Context window size                                                                                    \n",
    "min_word_count = 5   # Minimum word count                        \n",
    "sample = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "\n",
    "ft_model = FastText(tokenized_corpus, size=feature_size, window=window_context, \n",
    "                    min_count=min_word_count,sample=sample, sg=1, iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.054465</td>\n",
       "      <td>0.017652</td>\n",
       "      <td>0.057126</td>\n",
       "      <td>0.328237</td>\n",
       "      <td>-0.085430</td>\n",
       "      <td>0.128090</td>\n",
       "      <td>-0.086746</td>\n",
       "      <td>0.079687</td>\n",
       "      <td>0.019102</td>\n",
       "      <td>-0.012940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035275</td>\n",
       "      <td>-0.162782</td>\n",
       "      <td>-0.121593</td>\n",
       "      <td>-0.067105</td>\n",
       "      <td>0.088611</td>\n",
       "      <td>0.066156</td>\n",
       "      <td>-0.135727</td>\n",
       "      <td>0.007506</td>\n",
       "      <td>0.036671</td>\n",
       "      <td>0.186657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.108472</td>\n",
       "      <td>-0.185452</td>\n",
       "      <td>0.107285</td>\n",
       "      <td>0.114326</td>\n",
       "      <td>-0.065444</td>\n",
       "      <td>-0.065317</td>\n",
       "      <td>-0.228195</td>\n",
       "      <td>-0.001598</td>\n",
       "      <td>-0.077891</td>\n",
       "      <td>-0.039719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026372</td>\n",
       "      <td>0.040735</td>\n",
       "      <td>0.007004</td>\n",
       "      <td>0.139331</td>\n",
       "      <td>0.387488</td>\n",
       "      <td>0.096257</td>\n",
       "      <td>-0.070339</td>\n",
       "      <td>-0.179284</td>\n",
       "      <td>0.274889</td>\n",
       "      <td>0.060320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.017021</td>\n",
       "      <td>-0.194911</td>\n",
       "      <td>-0.071974</td>\n",
       "      <td>0.174593</td>\n",
       "      <td>0.058325</td>\n",
       "      <td>0.328171</td>\n",
       "      <td>-0.209054</td>\n",
       "      <td>0.219576</td>\n",
       "      <td>-0.153309</td>\n",
       "      <td>-0.097044</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.157658</td>\n",
       "      <td>-0.128863</td>\n",
       "      <td>-0.023152</td>\n",
       "      <td>0.032851</td>\n",
       "      <td>0.091259</td>\n",
       "      <td>0.093140</td>\n",
       "      <td>-0.142034</td>\n",
       "      <td>-0.089778</td>\n",
       "      <td>-0.012982</td>\n",
       "      <td>0.055193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.011983</td>\n",
       "      <td>-0.331382</td>\n",
       "      <td>-0.343084</td>\n",
       "      <td>0.271919</td>\n",
       "      <td>0.081006</td>\n",
       "      <td>0.382438</td>\n",
       "      <td>-0.037547</td>\n",
       "      <td>0.123154</td>\n",
       "      <td>-0.181144</td>\n",
       "      <td>0.021704</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.251193</td>\n",
       "      <td>-0.187517</td>\n",
       "      <td>0.031942</td>\n",
       "      <td>0.032144</td>\n",
       "      <td>0.177956</td>\n",
       "      <td>-0.154777</td>\n",
       "      <td>-0.117057</td>\n",
       "      <td>-0.097416</td>\n",
       "      <td>0.136062</td>\n",
       "      <td>-0.119283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.002666</td>\n",
       "      <td>0.123784</td>\n",
       "      <td>0.194492</td>\n",
       "      <td>0.523104</td>\n",
       "      <td>-0.138276</td>\n",
       "      <td>0.271805</td>\n",
       "      <td>-0.134554</td>\n",
       "      <td>0.010373</td>\n",
       "      <td>0.024799</td>\n",
       "      <td>0.136936</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045197</td>\n",
       "      <td>-0.227423</td>\n",
       "      <td>-0.236232</td>\n",
       "      <td>-0.178541</td>\n",
       "      <td>0.120102</td>\n",
       "      <td>-0.193745</td>\n",
       "      <td>-0.050619</td>\n",
       "      <td>0.034531</td>\n",
       "      <td>0.211434</td>\n",
       "      <td>0.149790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51259</td>\n",
       "      <td>-0.135934</td>\n",
       "      <td>-0.147382</td>\n",
       "      <td>-0.028985</td>\n",
       "      <td>0.166438</td>\n",
       "      <td>-0.080925</td>\n",
       "      <td>0.183791</td>\n",
       "      <td>-0.087949</td>\n",
       "      <td>0.391691</td>\n",
       "      <td>0.198972</td>\n",
       "      <td>-0.177645</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.123869</td>\n",
       "      <td>-0.031222</td>\n",
       "      <td>-0.068465</td>\n",
       "      <td>0.014227</td>\n",
       "      <td>-0.032453</td>\n",
       "      <td>0.005247</td>\n",
       "      <td>-0.126470</td>\n",
       "      <td>0.051706</td>\n",
       "      <td>-0.011188</td>\n",
       "      <td>0.050060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51260</td>\n",
       "      <td>0.087971</td>\n",
       "      <td>-0.177188</td>\n",
       "      <td>0.314846</td>\n",
       "      <td>0.219543</td>\n",
       "      <td>0.402228</td>\n",
       "      <td>0.164714</td>\n",
       "      <td>-0.369832</td>\n",
       "      <td>0.347020</td>\n",
       "      <td>-0.193511</td>\n",
       "      <td>0.002096</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.133168</td>\n",
       "      <td>-0.128057</td>\n",
       "      <td>-0.018000</td>\n",
       "      <td>-0.002734</td>\n",
       "      <td>-0.001974</td>\n",
       "      <td>0.280644</td>\n",
       "      <td>0.032183</td>\n",
       "      <td>-0.095871</td>\n",
       "      <td>-0.100626</td>\n",
       "      <td>0.368169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51261</td>\n",
       "      <td>0.150508</td>\n",
       "      <td>-0.241392</td>\n",
       "      <td>-0.081709</td>\n",
       "      <td>0.140822</td>\n",
       "      <td>0.508776</td>\n",
       "      <td>0.590009</td>\n",
       "      <td>-0.199491</td>\n",
       "      <td>-0.006691</td>\n",
       "      <td>-0.281000</td>\n",
       "      <td>-0.033489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079114</td>\n",
       "      <td>0.114753</td>\n",
       "      <td>0.156202</td>\n",
       "      <td>0.163266</td>\n",
       "      <td>0.277102</td>\n",
       "      <td>-0.017011</td>\n",
       "      <td>-0.196052</td>\n",
       "      <td>-0.064119</td>\n",
       "      <td>-0.009067</td>\n",
       "      <td>0.028108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51262</td>\n",
       "      <td>-0.068539</td>\n",
       "      <td>-0.162230</td>\n",
       "      <td>0.220101</td>\n",
       "      <td>0.049081</td>\n",
       "      <td>0.099600</td>\n",
       "      <td>-0.107287</td>\n",
       "      <td>-0.380536</td>\n",
       "      <td>0.260807</td>\n",
       "      <td>-0.045248</td>\n",
       "      <td>-0.059895</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.130126</td>\n",
       "      <td>-0.040220</td>\n",
       "      <td>-0.242087</td>\n",
       "      <td>0.235974</td>\n",
       "      <td>-0.000643</td>\n",
       "      <td>0.059560</td>\n",
       "      <td>-0.175171</td>\n",
       "      <td>-0.130933</td>\n",
       "      <td>-0.028605</td>\n",
       "      <td>0.019565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51263</td>\n",
       "      <td>-0.264979</td>\n",
       "      <td>0.071487</td>\n",
       "      <td>-0.075076</td>\n",
       "      <td>0.057371</td>\n",
       "      <td>-0.052829</td>\n",
       "      <td>0.281428</td>\n",
       "      <td>-0.084081</td>\n",
       "      <td>0.094206</td>\n",
       "      <td>-0.043663</td>\n",
       "      <td>-0.008208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011638</td>\n",
       "      <td>-0.153657</td>\n",
       "      <td>0.070342</td>\n",
       "      <td>0.134474</td>\n",
       "      <td>-0.071769</td>\n",
       "      <td>-0.091065</td>\n",
       "      <td>-0.357130</td>\n",
       "      <td>0.060380</td>\n",
       "      <td>-0.153551</td>\n",
       "      <td>-0.110389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51264 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "0     -0.054465  0.017652  0.057126  0.328237 -0.085430  0.128090 -0.086746   \n",
       "1     -0.108472 -0.185452  0.107285  0.114326 -0.065444 -0.065317 -0.228195   \n",
       "2     -0.017021 -0.194911 -0.071974  0.174593  0.058325  0.328171 -0.209054   \n",
       "3      0.011983 -0.331382 -0.343084  0.271919  0.081006  0.382438 -0.037547   \n",
       "4     -0.002666  0.123784  0.194492  0.523104 -0.138276  0.271805 -0.134554   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "51259 -0.135934 -0.147382 -0.028985  0.166438 -0.080925  0.183791 -0.087949   \n",
       "51260  0.087971 -0.177188  0.314846  0.219543  0.402228  0.164714 -0.369832   \n",
       "51261  0.150508 -0.241392 -0.081709  0.140822  0.508776  0.590009 -0.199491   \n",
       "51262 -0.068539 -0.162230  0.220101  0.049081  0.099600 -0.107287 -0.380536   \n",
       "51263 -0.264979  0.071487 -0.075076  0.057371 -0.052829  0.281428 -0.084081   \n",
       "\n",
       "             7         8         9   ...        90        91        92  \\\n",
       "0      0.079687  0.019102 -0.012940  ...  0.035275 -0.162782 -0.121593   \n",
       "1     -0.001598 -0.077891 -0.039719  ...  0.026372  0.040735  0.007004   \n",
       "2      0.219576 -0.153309 -0.097044  ... -0.157658 -0.128863 -0.023152   \n",
       "3      0.123154 -0.181144  0.021704  ... -0.251193 -0.187517  0.031942   \n",
       "4      0.010373  0.024799  0.136936  ... -0.045197 -0.227423 -0.236232   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "51259  0.391691  0.198972 -0.177645  ... -0.123869 -0.031222 -0.068465   \n",
       "51260  0.347020 -0.193511  0.002096  ... -0.133168 -0.128057 -0.018000   \n",
       "51261 -0.006691 -0.281000 -0.033489  ...  0.079114  0.114753  0.156202   \n",
       "51262  0.260807 -0.045248 -0.059895  ... -0.130126 -0.040220 -0.242087   \n",
       "51263  0.094206 -0.043663 -0.008208  ...  0.011638 -0.153657  0.070342   \n",
       "\n",
       "             93        94        95        96        97        98        99  \n",
       "0     -0.067105  0.088611  0.066156 -0.135727  0.007506  0.036671  0.186657  \n",
       "1      0.139331  0.387488  0.096257 -0.070339 -0.179284  0.274889  0.060320  \n",
       "2      0.032851  0.091259  0.093140 -0.142034 -0.089778 -0.012982  0.055193  \n",
       "3      0.032144  0.177956 -0.154777 -0.117057 -0.097416  0.136062 -0.119283  \n",
       "4     -0.178541  0.120102 -0.193745 -0.050619  0.034531  0.211434  0.149790  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "51259  0.014227 -0.032453  0.005247 -0.126470  0.051706 -0.011188  0.050060  \n",
       "51260 -0.002734 -0.001974  0.280644  0.032183 -0.095871 -0.100626  0.368169  \n",
       "51261  0.163266  0.277102 -0.017011 -0.196052 -0.064119 -0.009067  0.028108  \n",
       "51262  0.235974 -0.000643  0.059560 -0.175171 -0.130933 -0.028605  0.019565  \n",
       "51263  0.134474 -0.071769 -0.091065 -0.357130  0.060380 -0.153551 -0.110389  \n",
       "\n",
       "[51264 rows x 100 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector\n",
    "    \n",
    "\n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "# get document level embeddings\n",
    "ft_feature_array = averaged_word_vectorizer(corpus=tokenized_corpus, model=ft_model,\n",
    "                                             num_features=feature_size)\n",
    "tweet_ft=pd.DataFrame(ft_feature_array)\n",
    "tweet_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_ft.to_csv(\"FastText.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetft_df=pd.concat([tweet_ft,tweets_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x=tweetft_df.drop(['tweet','label'],axis=1)\n",
    "y=tweetft_df['label']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6980337078651685"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "naive_bayes = BernoulliNB()\n",
    "model = naive_bayes.fit(x_train, y_train)\n",
    "y_predictions = model.predict(x_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='newton-cg', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25,random_state = 42) \n",
    "log_classifier = LogisticRegression(multi_class='multinomial',solver ='newton-cg')\n",
    "log_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8224875156054932"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_classifier.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3887,  809,  143],\n",
       "       [ 791, 3689,  220],\n",
       "       [ 129,  183, 2965]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3874,  795,  170],\n",
       "       [ 794, 3664,  242],\n",
       "       [ 123,  176, 2978]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "model = LinearSVC()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8205368289637952"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gop': ['republican',\n",
       "  'republicans',\n",
       "  'senators',\n",
       "  'goptaxscam',\n",
       "  'jecrepublicans'],\n",
       " 'dem': ['dems', 'democrat', 'democratic', 'democrats', 'chair'],\n",
       " 'vote': ['voting',\n",
       "  'electionsmatter',\n",
       "  'ballot',\n",
       "  'washingtonprimary',\n",
       "  'registered'],\n",
       " 'attack': ['attacks', 'disgraced', 'frightening', 'antisemitism', 'waged'],\n",
       " 'administration': ['administrations',\n",
       "  'admin',\n",
       "  'trump',\n",
       "  'cruelly',\n",
       "  'president'],\n",
       " 'voters': ['ballot', 'polls', 'republicans', 'election', 'toxic'],\n",
       " 'clinton': ['hillary',\n",
       "  'clintons',\n",
       "  'hillaryclinton',\n",
       "  'mishandling',\n",
       "  'professors'],\n",
       " 'win': ['chance', 'compete', 'huge', 'geaux', 'winner'],\n",
       " 'donald trump': ['trump',\n",
       "  'donald',\n",
       "  'realdonaldtrump',\n",
       "  'president',\n",
       "  'enablers']}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words = {search_term: [item[0] for item in ft_model.wv.most_similar([search_term], topn=5)]\n",
    "                  for search_term in ['gop', 'dem', 'vote', 'attack', 'administration', 'voters', 'clinton','win','donald trump']}\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
